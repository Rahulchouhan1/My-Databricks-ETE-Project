{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24be4754-5711-456a-8300-5cd3b195fa21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f1c4c70-841a-42b7-a3ff-2bd666da2e62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Read The Customer data From the silver Layer and then Apply The SCD Type 1 and make this table as Dimension Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73431fe0-bd9e-4abf-8f84-9ac3339da799",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gold_customer_path = \"/Volumes/sampleproject/practice/gold/dim_customers\"\n",
    "source_customers = spark.read.format('delta').load(f\"/Volumes/sampleproject/practice/silver/customers\")\n",
    "\n",
    "is_gold_table_exists = DeltaTable.isDeltaTable(spark, gold_customer_path)\n",
    "\n",
    "if not is_gold_table_exists:\n",
    "\n",
    "    window_spec = Window.orderBy(\"customer_id\")\n",
    "    df_with_sk = source_customers.withColumn(\"customer_sk\", row_number().over(window_spec))\n",
    "    \n",
    "    df_with_sk.write.format(\"delta\").mode(\"overwrite\").save(gold_customer_path)\n",
    "    print(\"First run complete. Dimension created.\")\n",
    "\n",
    "else:\n",
    "    target_table = DeltaTable.forPath(spark, gold_customer_path)\n",
    "    target_df = target_table.toDF()\n",
    "    \n",
    "    max_sk_row = target_df.agg(max(\"customer_sk\")).collect()[0][0]\n",
    "    max_sk = max_sk_row if max_sk_row is not None else 0\n",
    "    \n",
    "    new_rows = source_customers.join(target_df, \"customer_id\", \"left_anti\")\n",
    "\n",
    "    window_spec = Window.orderBy(\"customer_id\")\n",
    "    new_rows_with_sk = new_rows.withColumn(\"customer_sk\", lit(max_sk) + row_number().over(window_spec))\n",
    "    \n",
    "    existing_rows = source_customers.join(target_df, \"customer_id\", \"inner\").select(source_customers[\"*\"])\n",
    "    existing_rows_staged = existing_rows.withColumn(\"customer_sk\", lit(-1))\n",
    "    \n",
    "    staging_df = new_rows_with_sk.unionByName(existing_rows_staged)\n",
    "    \n",
    "    target_table.alias(\"target\") \\\n",
    "        .merge(\n",
    "            staging_df.alias(\"source\"),\n",
    "            \"target.customer_id = source.customer_id\"\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set={\n",
    "            \"target.email\": \"source.email\",\n",
    "            \"target.city\": \"source.city\",\n",
    "            \"target.state\": \"source.state\",\n",
    "            \"target.full_name\": \"source.full_name\",\n",
    "            \"target.email_domain\": \"source.email_domain\",\n",
    "        }) \\\n",
    "        .whenNotMatchedInsertAll() \\\n",
    "        .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52ac0e58-7235-4255-819b-4d426044bf70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Read The Products data From the silver Layer and then Apply The SCD Type 2 and make this table as Dimension Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86fdadcb-2391-4156-876d-3dc777effb80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim_Products_SCD2 created successfully.\n"
     ]
    }
   ],
   "source": [
    "gold_path_products_scd2 = \"/Volumes/sampleproject/practice/gold/dim_products\"\n",
    "source_df = spark.read.format('delta').load(f\"/Volumes/sampleproject/practice/silver/products\")  \n",
    "\n",
    "if not DeltaTable.isDeltaTable(spark, gold_path_products_scd2):\n",
    "    \n",
    "    df_first_run = source_df \\\n",
    "        .withColumn(\"start_date\", current_timestamp()) \\\n",
    "        .withColumn(\"end_date\", lit(None).cast(\"timestamp\")) \\\n",
    "        .withColumn(\"is_current\", lit(True))\n",
    "        \n",
    "    window_spec = Window.orderBy(\"product_id\")\n",
    "    df_with_sk = df_first_run.withColumn(\"product_sk\", row_number().over(window_spec))\n",
    "    \n",
    "    df_with_sk.write.format(\"delta\").mode(\"overwrite\").save(gold_path_products_scd2)\n",
    "    print(\"Dim_Products_SCD2 created successfully.\")\n",
    "\n",
    "else:\n",
    "    \n",
    "    target_table = DeltaTable.forPath(spark, gold_path_products_scd2)\n",
    "    target_df = target_table.toDF()\n",
    "\n",
    "    max_sk_row = target_df.agg(max(\"product_sk\")).collect()[0][0]\n",
    "    max_sk = max_sk_row if max_sk_row is not None else 0\n",
    "\n",
    "\n",
    "    current_target = target_df.filter(col(\"is_current\") == True)\\\n",
    "\n",
    "    join_df = source_df.alias(\"src\").join(\n",
    "        current_target.alias(\"tgt\"),\n",
    "        on=\"product_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    rows_to_insert = join_df.filter(\n",
    "        (col(\"tgt.product_id\").isNull()) | \n",
    "        (col(\"src.price\") != col(\"tgt.price\")) |\n",
    "        (col(\"src.price_tier\") != col(\"tgt.price_tier\")) |\n",
    "        (col(\"src.data_quality_flag\") != col(\"tgt.data_quality_flag\"))\n",
    "    ).select(\"src.*\")\n",
    "    \n",
    "    rows_to_close = join_df.filter(\n",
    "        (col(\"tgt.product_id\").isNotNull()) & (\n",
    "            (col(\"src.price\") != col(\"tgt.price\")) |\n",
    "            (col(\"src.price_tier\") != col(\"tgt.price_tier\")) |\n",
    "            (col(\"src.data_quality_flag\") != col(\"tgt.data_quality_flag\"))\n",
    "        )\n",
    "    ).select(\"tgt.*\") \n",
    "   \n",
    "    window_spec_new = Window.orderBy(\"product_id\")\n",
    "    rows_to_insert_with_sk = rows_to_insert \\\n",
    "        .withColumn(\"product_sk\", lit(max_sk) + row_number().over(window_spec_new)) \\\n",
    "        .withColumn(\"start_date\", current_timestamp()) \\\n",
    "        .withColumn(\"end_date\", lit(None).cast(\"timestamp\")) \\\n",
    "        .withColumn(\"is_current\", lit(True)) \\\n",
    "        .withColumn(\"mergeKey\", col(\"product_sk\"))\n",
    "        \n",
    "    rows_to_close_prepared = rows_to_close \\\n",
    "        .withColumn(\"mergeKey\", col(\"product_sk\")) \\\n",
    "        .select(\"product_sk\", \"mergeKey\") \n",
    "\n",
    "    for col_name in rows_to_insert_with_sk.columns:\n",
    "        if col_name not in [\"product_sk\", \"mergeKey\"]:\n",
    "            rows_to_close_prepared = rows_to_close_prepared.withColumn(col_name, lit(None))\n",
    "            \n",
    "    staging_df = rows_to_insert_with_sk.unionByName(rows_to_close_prepared)\n",
    "    \n",
    "    target_table.alias(\"target\") \\\n",
    "        .merge(\n",
    "            staging_df.alias(\"source\"),\n",
    "            \"target.product_sk = source.mergeKey\"\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(\n",
    "            condition=\"target.is_current = true\", \n",
    "            set={\n",
    "                \"target.is_current\": lit(False),\n",
    "                \"target.end_date\": current_timestamp()\n",
    "            }\n",
    "        ) \\\n",
    "        .whenNotMatchedInsertAll() \\\n",
    "        .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b46d7b51-5ee0-41f5-a00b-98d03ba14c98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Read The Regions data From the silver Layer and then Apply The SCD Type 1 and make this table as Dimension Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26411732-75bd-48f3-97d0-e9261f1ab676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold Region table does not exist. Creating first version...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gold_path_regions = \"/Volumes/sampleproject/practice/gold/dim_regions\"\n",
    "source_df_regions = spark.read.format('delta').load(f\"/Volumes/sampleproject/practice/silver/regions\")\n",
    "\n",
    "if not DeltaTable.isDeltaTable(spark, gold_path_regions):\n",
    "    print(\"Gold Region table does not exist. Creating first version...\")\n",
    "    \n",
    "    window_spec = Window.orderBy(\"region_id\")\n",
    "    df_with_sk = source_df_regions.withColumn(\"region_sk\", row_number().over(window_spec))\n",
    "    \n",
    "    df_with_sk.write.format(\"delta\").mode(\"overwrite\").save(gold_path_regions)\n",
    "\n",
    "else:\n",
    "    target_table = DeltaTable.forPath(spark, gold_path_regions)\n",
    "    target_df = target_table.toDF()\n",
    "\n",
    "    max_sk_row = target_df.agg(max(\"region_sk\")).collect()[0][0]\n",
    "    max_sk = max_sk_row if max_sk_row is not None else 0\n",
    "    \n",
    "    new_rows = source_df_regions.join(target_df, \"region_id\", \"left_anti\")\n",
    "    \n",
    "    window_spec = Window.orderBy(\"region_id\")\n",
    "    new_rows_with_sk = new_rows.withColumn(\"region_sk\", lit(max_sk) + row_number().over(window_spec))\n",
    "    \n",
    "    existing_rows = source_df_regions.join(target_df, \"region_id\", \"inner\").select(source_df_regions[\"*\"])\n",
    "    existing_rows_staged = existing_rows.withColumn(\"region_sk\", lit(-1))\n",
    "    \n",
    "    staging_df = new_rows_with_sk.unionByName(existing_rows_staged)\n",
    "    \n",
    "    target_table.alias(\"target\") \\\n",
    "        .merge(\n",
    "            staging_df.alias(\"source\"),\n",
    "            \"target.region_id = source.region_id\"\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set={\n",
    "            \"target.region\": \"source.region\",\n",
    "            \"target.region_code\": \"source.region_code\",\n",
    "            \"target.zone_type\": \"source.zone_type\"\n",
    "        }) \\\n",
    "        .whenNotMatchedInsertAll() \\\n",
    "        .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7799838-7c6c-4d0f-8fc4-6e45756207e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##  **Create Order Table As Fact Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8adb095-7ade-4ea5-9738-96d1e4f9eedb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_fact_path = \"/Volumes/sampleproject/practice/gold/fact_orders\"\n",
    "\n",
    "df = spark.read.format(\"delta\").load(\"/Volumes/sampleproject/practice/silver/orders\")\n",
    "\n",
    "df_dimcus = spark.read.format(\"delta\").load(\"/Volumes/sampleproject/practice/gold/dim_customers\") \\\n",
    "    .withColumnRenamed(\"customer_id\", \"dim_customer_id\") \n",
    "\n",
    "df_dimpro = spark.read.format(\"delta\").load(\"/Volumes/sampleproject/practice/gold/dim_products\") \\\n",
    "    .withColumnRenamed(\"product_id\", \"dim_product_id\")\n",
    "\n",
    "df_fact = df.join(\n",
    "    df_dimcus, \n",
    "    df['customer_id'] == df_dimcus['dim_customer_id'], \n",
    "    how='left'\n",
    ").join(\n",
    "    df_dimpro, \n",
    "    df['product_id'] == df_dimpro['dim_product_id'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_fact_new = df_fact.drop('dim_customer_id', 'dim_product_id', 'customer_id', 'product_id','_rescued_data')\n",
    "\n",
    "if DeltaTable.isDeltaTable(spark, gold_fact_path):\n",
    "    \n",
    "    dlt_obj = DeltaTable.forPath(spark, gold_fact_path)\n",
    "\n",
    "    dlt_obj.alias(\"trg\").merge(\n",
    "        df_fact_new.alias(\"src\"), \n",
    "        \"trg.order_id = src.order_id AND trg.customer_sk = src.customer_sk AND trg.product_sk = src.product_sk\"\n",
    "    )\\\n",
    "    .whenMatchedUpdateAll()\\\n",
    "    .whenNotMatchedInsertAll()\\\n",
    "    .execute()\n",
    "\n",
    "else:\n",
    "    df_fact_new.select('order_id','order_date','quantity','total_amount','year','customer_sk','product_sk').write.format(\"delta\").mode(\"overwrite\").save(gold_fact_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de3ff78c-5aac-43ca-b405-8d429f22891d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-8799344854048558>, line 10\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    CREATE OR REPLACE VIEW sampleproject.powerbi.dim_regions AS\u001B[0m\n",
       "\u001B[0m           ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "SyntaxError",
        "evalue": "invalid syntax (command-8799344854048558-997059518, line 10)"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>SyntaxError</span>: invalid syntax (command-8799344854048558-997059518, line 10)\n[Trace ID: 00-552c5ec0abfd225a611f2d9df5dd2746-053979c41aad5bff-00]"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;36m  File \u001B[0;32m<command-8799344854048558>, line 10\u001B[0;36m\u001B[0m\n\u001B[0;31m    CREATE OR REPLACE VIEW sampleproject.powerbi.dim_regions AS\u001B[0m\n\u001B[0m           ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS sampleproject.powerbi;\n",
    "\n",
    "CREATE OR REPLACE VIEW sampleproject.powerbi.dim_customers AS \n",
    "SELECT * FROM delta.`/Volumes/sampleproject/practice/gold/dim_customers`;\n",
    "\n",
    "CREATE OR REPLACE VIEW sampleproject.powerbi.dim_products AS \n",
    "SELECT * FROM delta.`/Volumes/sampleproject/practice/gold/dim_products`;\n",
    "\n",
    "CREATE OR REPLACE VIEW sampleproject.powerbi.dim_regions AS \n",
    "SELECT * FROM delta.`/Volumes/sampleproject/practice/gold/dim_regions`;\n",
    "\n",
    "CREATE OR REPLACE VIEW sampleproject.powerbi.fact_orders AS \n",
    "SELECT * FROM delta.`/Volumes/sampleproject/practice/gold/fact_orders`;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb5a60ea-f5e4-4c04-be80-4237e0faecc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8799344854048585,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold_Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}